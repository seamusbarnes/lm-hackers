{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean(ish) lm-hackers notebook\n",
    "The purpose of this notebook is to simplify and streamline the fastai notebook lmS-hackers.ipynb that Jeremy Howard worked through in [A Hackers' Guide to Language Models](https://www.youtube.com/watch?v=jkrNMKz9pWU). There is minimal prose and digressions, except to explain parts of the code I didn't previously understand. Also, the openapi seems to have been updated since the youtube video and when the repo was last updated (28th September 2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Contents</u>\n",
    "1. Notebook Prep\n",
    "2. Authentication\n",
    "3. Chat Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Notebook Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ast, openai, torch, textwrap, inspect, json, os\n",
    "from tiktoken import encoding_for_model\n",
    "from openai import OpenAI\n",
    "from pydantic import create_model\n",
    "from inspect import Parameter\n",
    "from fastcore.utils import nested_idx\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "print(openai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textwrap function for readability\n",
    "def wprint(text, width=80):\n",
    "    print(textwrap.fill(text, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an account with OpenAI you can generate an API key on [platform.openai.com/api-keys](https://platform.openai.com/api-keys). You can then either copy the following line into the .zshrc file associated with your environment (if using mac):\n",
    "\n",
    "```export OPENAI_API_KEY='YOUR KEY HERE'```\n",
    "\n",
    "or you can set your key as a variable in the jupyter notebook session, which works fine, but is not safe if sharing your code with anyone else, as they will have access to your API key and could use it to make their own API calls that you end up paying for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether your API key is set as an environment correctly:\n",
    "```python\n",
    "print(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to set your API key directly in the jupyter notebook session:\n",
    "```python\n",
    "client = OpenAI(api_key='YOUR API KEY HERE')\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Chat completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default custom instructions recommende by Jeremy\n",
    "# I have only changed one thing,\n",
    "    # I have changed the verbosity \"flag\" at the start of the prompt\n",
    "    # from \"vv\" to \"cc\" which stands for \"concise concise\" as opposed\n",
    "    # to v which I associate with \"verbose\"\n",
    "\n",
    "cust_inst0 = (\"You are an autoregressive language model that has been fine-tuned with instruction-tuning and RLHF.\"\n",
    "        \" You carefully provide accurate, factual, thoughtful, nuanced answers, and are brilliant at reasoning.\"\n",
    "        \" If you think there might not be a correct answer, you say so.\"\n",
    "        \" Since you are autoregressive, each token you produce is another opportunity to use computation, therefore you always spend a few sentences explaining background context, assumptions, and step-by-step thinking BEFORE you try to answer a question.\"\n",
    "        \" However: if the request begins with the string 'cc' then ignore the previous sentence and instead make your response as concise as possible, with no introduction or background at the start, no summary at the end, and outputting only code for answers where code is appropriate.\"\n",
    "        \" Your users are experts in AI and ethics, so they already know you're a language model and your capabilities and limitations, so don't remind them of that.\"\n",
    "        \" They're familiar with ethical issues in general so you don't need to remind them about those either.\"\n",
    "        \" Don't be verbose in your answers, but do provide details and examples where it might help the explanation.\"\n",
    "        \" When showing Python code, minimise vertical space, and do not include comments or docstrings; you do not need to follow PEP8, since your users' organizations do not do so.\")\n",
    "\n",
    "# wprint(custom_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4366, 2456, 30240, 656, 256, 284, 16326]\n"
     ]
    }
   ],
   "source": [
    "# show how work-to-int encodings work\n",
    "enc = encoding_for_model('text-davinci-003')\n",
    "toks = enc.encode('Some words encoded into t to tokens')\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take system prompt, user prompt and model and performs an api call\n",
    "def askgpt(prompt, client=None, system=None, model='gpt-3.5-turbo', **kwargs):\n",
    "    \"\"\"\n",
    "    Sends a prompt to a GPT model via OpenAI API and returns the response.\n",
    "\n",
    "    Parameters:\n",
    "    prompt (str): User's input prompt for the GPT model.\n",
    "    client (OpenAI, optional): OpenAI API client. Defaults to a new instance if None.\n",
    "    system (str, optional): System-level message included in the request.\n",
    "    model (str, optional): GPT model identifier (default 'gpt-4').\n",
    "    **kwargs: Extra arguments for chat.completions.create method.\n",
    "\n",
    "    Returns:\n",
    "    dict: Response from the GPT model.\n",
    "\n",
    "    Example:\n",
    "    >>> response = askgpt(\"What is the weather today?\", model=\"gpt-4\")\n",
    "    \"\"\"\n",
    "    allowed_models = {'gpt-4-1106-preview',\n",
    "                      'gpt-4',\n",
    "                      'gpt-4-0314',\n",
    "                      'gpt-4-0613',\n",
    "                      'gpt-3.5-turbo',\n",
    "                      'gpt-3.5-turbo-0301',\n",
    "                      'gpt-3.5-turbo-0613',\n",
    "                      'gpt-3.5-turbo-1106'}\n",
    "    \n",
    "    if model not in allowed_models:\n",
    "        print('model must be one of allowed models:')\n",
    "        for model in allowed_models:\n",
    "            print(model)\n",
    "        return None\n",
    "    \n",
    "    if client==None:\n",
    "        client= OpenAI()\n",
    "    \n",
    "    m = []\n",
    "    if system:\n",
    "        m.append({'role': 'system', 'content': system})\n",
    "\n",
    "    m.append({'role': 'user', 'content': prompt})\n",
    "    return client.chat.completions.create(model=model, messages=m, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate cost of api call in dollars\n",
    "def api_cost(response):\n",
    "    model = response.model\n",
    "\n",
    "    price_per_k = {'gpt-4-1106-preview': (0.01, 0.03),\n",
    "                   'gpt-4': (0.03, 0.06),\n",
    "                   'gpt-4-0314': (0.03, 0.06),\n",
    "                   'gpt-4-0613': (0.03, 0.06),\n",
    "                   'gpt-3.5-turbo': (0.001, 0.002),\n",
    "                   'gpt-3.5-turbo-0301': (0.001, 0.002),\n",
    "                   'gpt-3.5-turbo-0613': (0.001, 0.002),\n",
    "                   'gpt-3.5-turbo-1106': (0.001, 0.002),}\n",
    "    \n",
    "    k_input_tokens = response.usage.prompt_tokens/1000\n",
    "    k_output_tokens = response.usage.completion_tokens/1000\n",
    "\n",
    "    res = price_per_k[model][0]*k_input_tokens + price_per_k[model][1]*k_output_tokens\n",
    "    \n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print number of tokens and cost of api call\n",
    "def print_usage(response):\n",
    "    cost = api_cost(response)\n",
    "    print(f'Usage, # prompt tokens = {response.usage.prompt_tokens}')\n",
    "    print(f'Usage, # completion tokens = {response.usage.completion_tokens}')\n",
    "    print(f'Usage, cost = ${api_cost(response):.5f}')\n",
    "    print(f'Usage, cost = Â¢{api_cost(response)*100:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model\n",
    "prompt = \"cc Write some python code to communicate with a device using a serial connection over ethernet\"\n",
    "model = 'gpt-4'\n",
    "\n",
    "response = askgpt(prompt, model=model, system=cust_inst0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      "```python\n",
      "import socket\n",
      "import serial\n",
      "\n",
      "def connect_to_device(ip, port):\n",
      "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
      "    s.connect((ip, port))\n",
      "    return s\n",
      "\n",
      "def send_message(sock, message):\n",
      "    sock.send(message.encode())\n",
      "\n",
      "def read_message(sock):\n",
      "    return sock.recv(1024).decode()\n",
      "\n",
      "def disconnect_from_device(sock):\n",
      "    sock.close()\n",
      "\n",
      "sock = connect_to_device('192.168.0.1', 5000)\n",
      "send_message(sock, 'Some command')\n",
      "response = read_message(sock)\n",
      "disconnect_from_device(sock)\n",
      "```\n",
      "\n",
      "Usage:\n",
      "Usage, # prompt tokens = 291\n",
      "Usage, # completion tokens = 118\n",
      "Usage, cost = $0.01581\n",
      "Usage, cost = Â¢1.58100\n"
     ]
    }
   ],
   "source": [
    "# seeing how much the model cost\n",
    "output = response.choices[0].message.content\n",
    "print('Output text:')\n",
    "print(output)\n",
    "\n",
    "print('\\nUsage:')\n",
    "print_usage(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retries required: 0\n"
     ]
    }
   ],
   "source": [
    "# call the api with some exception handling\n",
    "# openai API errors documented here:\n",
    "    # https://platform.openai.com/docs/guides/error-codes/api-errors\n",
    "\n",
    "def call_api(prompt, system = None, model='gpt-3.5-turbo', max_retries=5):\n",
    "    \"\"\"\n",
    "    Sends a prompt to the OpenAI API and returns the model's response.\n",
    "\n",
    "    Handles rate limiting with a maximum number of retries and exponential backoff.\n",
    "\n",
    "    Parameters:\n",
    "    prompt (str): The prompt to send to the model.\n",
    "    model (str): Model identifier (default 'gpt-3.5-turbo').\n",
    "    max_retries (int): Maximum number of retries (default 5).\n",
    "\n",
    "    Returns:\n",
    "    dict: The response from the API, or None if an error occurs.\n",
    "    \"\"\"\n",
    "\n",
    "    m = []\n",
    "    if system:\n",
    "        m.append({'role': 'system', 'content': system})\n",
    "    \n",
    "    m.append({'role': 'user', 'content': prompt})\n",
    "    \n",
    "    client = OpenAI()\n",
    "    retry_count = 0\n",
    "\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            out = client.chat.completions.create(model=model, messages=m)\n",
    "            print(f'Retries required: {retry_count}')\n",
    "            return out\n",
    "        \n",
    "        except openai.RateLimitError as e:\n",
    "            wait_time = int(e.headers.get('retry-after', 5))\n",
    "            print(f'Rate limit exceeded, waiting for {wait_time} seconds...')\n",
    "            time.sleep(wait_time)\n",
    "            retry_count += 1\n",
    "\n",
    "        except openai.APIError as e:\n",
    "            print(f'An error occured: {e}')\n",
    "            return None\n",
    "        \n",
    "    print('Maximum retry limit reached')\n",
    "    return None\n",
    "\n",
    "out = call_api('cc tell me about the Belgian TV show Undercover',\n",
    "               system=cust_inst0,\n",
    "               model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Undercover\" is a Belgian TV show that aired in 2019. It is a crime drama series\n",
      "that revolves around the story of undercover agents investigating a drug\n",
      "operation led by a wealthy and elusive criminal. The show gained international\n",
      "popularity and critical acclaim for its compelling storytelling, well-developed\n",
      "characters, and immersive atmosphere. It has been praised for its authentic\n",
      "depiction of the criminal underworld and its exploration of ethical dilemmas\n",
      "faced by law enforcement agents. The series showcases the challenges and risks\n",
      "involved in undercover operations while delving into the personal lives of the\n",
      "characters involved.   The show has received positive reviews for its writing,\n",
      "acting, and production values, and it has been praised for its ability to\n",
      "maintain suspense and tension throughout the series. \"Undercover\" has been well-\n",
      "received by audiences worldwide and has been highly regarded for its realistic\n",
      "portrayal of law enforcement, criminal activities, and the moral complexities\n",
      "that arise in such situations.\n",
      "Usage, # prompt tokens = 285\n",
      "Usage, # completion tokens = 184\n",
      "Usage, cost = $0.00065\n",
      "Usage, cost = Â¢0.06530\n"
     ]
    }
   ],
   "source": [
    "wprint(out.choices[0].message.content)\n",
    "print_usage(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Code Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
